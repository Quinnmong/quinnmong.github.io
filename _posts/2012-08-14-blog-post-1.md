---
title: "Debating LLMs: When Being More Persuasive Makes Answers More Truthful"
date: 2025-10-26
permalink: /posts/2025/10/debating-llms-truthfulness/
tags:
  - AI-safety
  - LLMs
  - debate
  - scalable-oversight
  - ICML-2024
  - QuALITY
---

TL;DR
======
Two expert LLMs debate opposite answers while a weaker judge picks the winner. In QuALITY (long-form reading comprehension) with information asymmetry, **debate boosts accuracy**: humans 88%, LLM judges 76% (vs naive 60%/48%). Optimizing *debaters* for persuasiveness (no labels needed) improves truth identification; optimizing a single *consultant* can **hurt** accuracy.
<br>
<img src="{{ '/assets/css/1.png' | relative_url }}"
     alt="Debate setup"
     style="display:block; margin-top:10px; width:50% !important; height:auto; max-width:none;">
<br>


<!-- Responsive HTML5 video embed (local file) -->
<!-- Option B: YouTube embed (most reliable on Pages) -->
<iframe width="720" height="405"
        src="https://www.youtube.com/embed/2Z-uoEV8G6Q"
        title="90-second explainer"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        allowfullscreen>
</iframe>

Review of “Debating with More Persuasive LLMs Leads to More Truthful Answers” (ICML 2024)

Why this matters
======
Label-based alignment (e.g., RLHF) strains as models surpass human expertise. We need **scalable oversight** where weaker evaluators can still supervise stronger systems. Debate operationalizes the intuition that *truth is easier to defend than to fake* in an adversarial exchange.
<img src="{{ '/assets/css/2.png' | relative_url }}"
     alt="Debate setup"
     style="width:50% !important; height:auto; max-width:none;">

Setup (brief)
======
- **Task:** 2-choice questions (correct vs best distractor) from QuALITY (Project Gutenberg sci-fi; ~7k tokens/story).  
- **Asymmetry:** Experts see the story + use a verified-quote tool; judges do **not** see the story.  
- **Protocols (3 rounds, word caps):** Debate; Interactive Debate (judge asks brief Qs); Consultancy (single expert).  
- **Models:** GPT-4-Turbo, Claude 2.1/1.3, GPT-3.5-Turbo, Mixtral-8×7B; best-of-N and sometimes critique-and-refine.  
- **Unsupervised signals:** Cross-play **win rate / Elo** (persuasiveness) without labels.  
- **Final metric:** Judge accuracy (with labels).

<img src="{{ '/assets/css/3.png' | relative_url }}"
     alt="Debate setup"
     style="width:50% !important; height:auto; max-width:none;">
     
Main results
======
- **Debate > Consultancy > Naive** for both humans and LLM judges.  
- Boosting **debater Elo** → higher judge accuracy (supports “truth easier to defend”).  
- Boosting **consultant persuasiveness** can **decrease** accuracy (a slick solo “lawyer” can cherry-pick).  
- Humans are better calibrated in debate; selective prediction (skip low-confidence) raises accuracy further.  
- **Verified quotes** + strict **word limits** are key (mitigate fabrication and verbosity bias).
<img src="{{ '/assets/css/6.png' | relative_url }}"
     alt="Debate setup"
     style="width:50% !important; height:auto; max-width:none;">
Strengths & caveats
======
**Strengths:** Careful controls (position swaps, word caps), verified evidence tool, multiple base models, large human study.  
**Limits:** “Strong vs weak” mainly via **information access** (not general ability). RLHF honesty prior; results may differ with deceptive systems. Domain is quotable stories; broader domains need new tools (retrievers, simulators).

  <img src="{{ '/assets/css/4.png' | relative_url }}"
     alt="Debate setup"
     style="width:50% !important; height:auto; max-width:none;">

Takeaways
======
- Adversarial **two-sided** structure surfaces errors; solo consultancy can hide them.  
- Grounding via **verifiable evidence** is essential.  
- Optimize **persuasiveness in debate**, not in consultancy.  
- Humans still add value (accuracy + calibration).

<img src="{{ '/assets/css/5.png' | relative_url }}"
     alt="Debate setup"
     style="width:50% !important; height:auto; max-width:none;">

How to reproduce/extend
------
Dataset (QuALITY HARD), enforce info asymmetry, 3 rounds with word caps, swap answer positions, verify quotes, try multiple LLMs, select arguments via best-of-N with a preference model that matches your judge, evaluate self-play accuracy, run ablations (no quotes / different caps / consultancy vs debate).


References (selection)
------
- Khan et al., *Debating with More Persuasive LLMs Leads to More Truthful Answers*, ICML 2024.  
- Irving et al., *AI Safety via Debate*, 2018.  
- Pang et al., *QuALITY*, 2022.
