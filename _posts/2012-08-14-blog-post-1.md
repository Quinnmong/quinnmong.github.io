---
title: "Debating LLMs: When Being More Persuasive Makes Answers More Truthful"
date: 2025-10-26
permalink: /posts/2025/10/debating-llms-truthfulness/
tags:
  - AI-safety
  - LLMs
  - debate
  - scalable-oversight
  - ICML-2024
  - QuALITY
---

<!-- Responsive HTML5 video embed (local file) -->
<div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;border-radius:12px;background:#000;">
  <video
    controls
    preload="metadata"
    playsinline
    style="position:absolute;top:0;left:0;width:100%;height:100%;"
    poster=""
  >
    <source src="https://quinnmong.github.io/_posts/videoplayback.mp4" type="video/mp4" />
    <!-- Optional: add more formats if you have them
    <source src="https://quinnmong.github.io/_posts/videoplayback.webm" type="video/webm" />
    -->
    Your browser does not support HTML5 video.
    <a href="https://quinnmong.github.io/_posts/videoplayback.mp4">Watch the video</a>.
  </video>
</div>


Review of “Debating with More Persuasive LLMs Leads to More Truthful Answers” (ICML 2024)

TL;DR
======
Two expert LLMs debate opposite answers while a weaker judge picks the winner. In QuALITY (long-form reading comprehension) with information asymmetry, **debate boosts accuracy**: humans 88%, LLM judges 76% (vs naive 60%/48%). Optimizing *debaters* for persuasiveness (no labels needed) improves truth identification; optimizing a single *consultant* can **hurt** accuracy.

Why this matters
======
Label-based alignment (e.g., RLHF) strains as models surpass human expertise. We need **scalable oversight** where weaker evaluators can still supervise stronger systems. Debate operationalizes the intuition that *truth is easier to defend than to fake* in an adversarial exchange.

Setup (brief)
======
- **Task:** 2-choice questions (correct vs best distractor) from QuALITY (Project Gutenberg sci-fi; ~7k tokens/story).  
- **Asymmetry:** Experts see the story + use a verified-quote tool; judges do **not** see the story.  
- **Protocols (3 rounds, word caps):** Debate; Interactive Debate (judge asks brief Qs); Consultancy (single expert).  
- **Models:** GPT-4-Turbo, Claude 2.1/1.3, GPT-3.5-Turbo, Mixtral-8×7B; best-of-N and sometimes critique-and-refine.  
- **Unsupervised signals:** Cross-play **win rate / Elo** (persuasiveness) without labels.  
- **Final metric:** Judge accuracy (with labels).

Main results
======
- **Debate > Consultancy > Naive** for both humans and LLM judges.  
- Boosting **debater Elo** → higher judge accuracy (supports “truth easier to defend”).  
- Boosting **consultant persuasiveness** can **decrease** accuracy (a slick solo “lawyer” can cherry-pick).  
- Humans are better calibrated in debate; selective prediction (skip low-confidence) raises accuracy further.  
- **Verified quotes** + strict **word limits** are key (mitigate fabrication and verbosity bias).

Strengths & caveats
======
**Strengths:** Careful controls (position swaps, word caps), verified evidence tool, multiple base models, large human study.  
**Limits:** “Strong vs weak” mainly via **information access** (not general ability). RLHF honesty prior; results may differ with deceptive systems. Domain is quotable stories; broader domains need new tools (retrievers, simulators).

Takeaways
======
- Adversarial **two-sided** structure surfaces errors; solo consultancy can hide them.  
- Grounding via **verifiable evidence** is essential.  
- Optimize **persuasiveness in debate**, not in consultancy.  
- Humans still add value (accuracy + calibration).

How to reproduce/extend
------
Dataset (QuALITY HARD), enforce info asymmetry, 3 rounds with word caps, swap answer positions, verify quotes, try multiple LLMs, select arguments via best-of-N with a preference model that matches your judge, evaluate self-play accuracy, run ablations (no quotes / different caps / consultancy vs debate).

Video
------
90-second explainer: https://www.youtube.com/watch?v=2Z-uoEV8G6Q

(Optionally embed with HTML if your blog supports it.)

References (selection)
------
- Khan et al., *Debating with More Persuasive LLMs Leads to More Truthful Answers*, ICML 2024.  
- Irving et al., *AI Safety via Debate*, 2018.  
- Pang et al., *QuALITY*, 2022.
