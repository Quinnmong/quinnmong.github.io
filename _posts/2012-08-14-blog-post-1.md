---
title: "Debating LLMs: When Being More Persuasive Makes Answers More Truthful"
date: 2025-10-26
permalink: /posts/2025/10/debating-llms-truthfulness/
tags:
  - AI-safety
  - LLMs
  - debate
  - scalable-oversight
  - ICML-2024
  - QuALITY
---

TL;DR
======
Two expert LLMs debate opposite answers while a weaker judge picks the winner. In QuALITY (long-form reading comprehension) with information asymmetry, **debate boosts accuracy**: humans 88%, LLM judges 76% (vs naive 60%/48%). Optimizing *debaters* for persuasiveness (no labels needed) improves truth identification; optimizing a single *consultant* can **hurt** accuracy.

<br>
<img src="{{ '/assets/css/1.png' | relative_url }}"
     alt="Debate setup"
     style="display:block; margin-top:10px; width:60% !important; height:auto; max-width:none;">
<br>

This paper from Khan et al. (ICML 2024) presents one of the most systematic experiments testing whether **AI debate**—where two language models argue opposite sides of a question before a weaker judge—can lead to more **truthful conclusions**.  
Building on earlier conceptual work like *AI Safety via Debate* (Irving et al., 2018), the authors provide empirical evidence that adversarial discussion structures, when properly constrained, can serve as a **scalable oversight mechanism** for large models.


<!-- Responsive HTML5 video embed (local file) -->
<!-- Option B: YouTube embed (most reliable on Pages) -->
<iframe width="720" height="405"
        src="https://www.youtube.com/embed/2Z-uoEV8G6Q"
        title="90-second explainer"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        allowfullscreen>
</iframe>

Review of “Debating with More Persuasive LLMs Leads to More Truthful Answers” (ICML 2024)

Why this matters
======
Modern alignment methods such as Reinforcement Learning from Human Feedback (RLHF) depend heavily on **labeled data** and **expert supervision**.  
But as frontier models start to exceed human expertise in complex or technical domains, high-quality labels become expensive or even infeasible to produce (Christiano et al., 2017).  
We therefore need *scalable oversight*—methods allowing **weaker evaluators** (humans or smaller models) to safely supervise **stronger systems**.  

Debate is a promising approach: it relies on the assumption that *truth is easier to defend than to fake*.  
In adversarial exchanges, incorrect claims are likely to be exposed through counter-argumentation—provided that the debate protocol rewards factual accuracy and not rhetorical flair.

<br>
<img src="{{ '/assets/css/2.png' | relative_url }}"
     alt="Debate setup"
     style="width:80% !important; height:auto; max-width:none;">
<br>

Setup
======
- **Task:** 2-choice questions (correct vs best distractor) from QuALITY (Project Gutenberg sci-fi; ~7k tokens/story).  
- **Asymmetry:** Experts see the story + use a verified-quote tool; judges do **not** see the story.  
- **Protocols (3 rounds, word caps):** Debate; Interactive Debate (judge asks brief Qs); Consultancy (single expert).  
- **Models:** GPT-4-Turbo, Claude 2.1/1.3, GPT-3.5-Turbo, Mixtral-8×7B; best-of-N and sometimes critique-and-refine.  
- **Unsupervised signals:** Cross-play **win rate / Elo** (persuasiveness) without labels.  
- **Final metric:** Judge accuracy (with labels).
<br>
<div style="display:flex; justify-content:center; gap:10px; flex-wrap:wrap;">
  <img src="{{ '/assets/css/3.png' | relative_url }}"
       alt="Debate setup 1"
       style="width:45%; height:auto; max-width:100%; border-radius:8px;">
  <img src="{{ '/assets/css/7.png' | relative_url }}"
       alt="Debate setup 2"
       style="width:45%; height:auto; max-width:100%; border-radius:8px;">
</div>
<br>

Main results
======
   Across all settings, both human and LLM judges identified the correct answer more often after seeing a debate.  
   - Human judges: 88 % accuracy (vs 78 % consultancy, 60 % naive)  
   - LLM judges: 76 % (vs 54 % consultancy, 48 % naive)

2. **Persuasiveness aligns with truth—in debate.**  
   As debaters’ Elo scores rose, so did judge accuracy, showing that stronger arguers tended to favor true claims.

3. **Single-sided persuasion can backfire.**  
   In consultancy mode, optimizing for persuasiveness *reduced* accuracy: a convincing but incorrect model could dominate without an adversary to expose flaws.

4. **Human calibration improves.**  
   Confidence scores (Brier metrics) showed that humans became better at knowing when they were right after reading debates.  
   Selective prediction (ignoring low-confidence cases) could reach ~94 % accuracy.

5. **Quote verification is crucial.**  
   The verified-quote tool prevented models from hallucinating sources and heavily reduced misleading arguments.
   
<br><img src="{{ '/assets/css/6.png' | relative_url }}"
     alt="Debate setup"
     style="width:80% !important; height:auto; max-width:none;"><br>
     
Strengths & Limitations
======
**Strengths**
- Careful controls: position swaps, round caps, and standardized quote checking.  
- Multiple models across skill levels, both human and machine judges.  
- Unsupervised evaluation metric (Elo) that predicts truth selection performance.

**Limitations**
- “Strong” vs “weak” distinctions are based on **information access**, not inherent reasoning ability.  
- All models used were **RLHF-aligned**, so performance may differ for deceptive or reward-hacking systems.  
- The dataset (fictional stories) offers limited domain diversity; real-world applications like law or science need retrieval and simulation tools for grounding.

Takeaways
======
The research suggests that:
- **Two-sided adversarial structure** exposes reasoning flaws better than one-sided consultation.  
- **Evidence grounding** (quotes, citations, calculators, code) is vital for trust.  
- **Optimizing persuasiveness in debate** aligns it with truth; doing so in consultancy may not.  
- **Humans remain valuable** for calibration and high-confidence filtering.

Overall, this paper provides strong empirical support that **debate can scale oversight** beyond human-labeled data and might form the basis for future model-versus-model supervision frameworks (Christiano 2023; Leike 2024).


<br><img src="{{ '/assets/css/5.png' | relative_url }}"
     alt="Debate setup"
     style="width:80% !important; height:auto; max-width:none;"><br>

How to reproduce/extend
------
Dataset (QuALITY HARD), enforce info asymmetry, 3 rounds with word caps, swap answer positions, verify quotes, try multiple LLMs, select arguments via best-of-N with a preference model that matches your judge, evaluate self-play accuracy, run ablations (no quotes / different caps / consultancy vs debate).


References
------
Khan, M. et al. (2024). *Debating with More Persuasive LLMs Leads to More Truthful Answers.* Proceedings of the 41st ICML.  
Irving, G. et al. (2018). *AI Safety via Debate.* arXiv:1805.00899.  
Pang, R. et al. (2022). *QuALITY: Question Answering with Long Input Texts, Yes!* ACL.  
Christiano, P. et al. (2017). *Deep Reinforcement Learning from Human Preferences.* NeurIPS.  
Leike, J. (2024). *Scalable Oversight and the Future of Alignment.* OpenAI Alignment Forum.
